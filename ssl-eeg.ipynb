{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings  import *\n",
    "from preprocessing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(subject):\n",
    "    \"\"\"\n",
    "    a function to get preprocessed eeg signal for a single subject\n",
    "    \"\"\"\n",
    "    # Bandpass filtring \n",
    "    raw = get_raw(subject)\n",
    "    ica = get_ica(subject)\n",
    "    \n",
    "    return reconstuct_signal(raw, ica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /home/brain/openmiir/raw_data/P01-raw.fif...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 64)  idle\n",
      "    Range : 0 ... 2478165 =      0.000 ...  4840.166 secs\n",
      "Ready.\n",
      "Reading 0 ... 2478165  =      0.000 ...  4840.166 secs...\n",
      "Reading /home/brain/openmiir/eeg/preprocessing/ica/P01-100p_64c-ica.fif ...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 64)  idle\n",
      "Now restoring ICA solution ...\n",
      "Ready.\n",
      "Transforming to ICA space (61 components)\n",
      "Zeroing out 4 ICA components\n"
     ]
    }
   ],
   "source": [
    "X = process(\"01\")\n",
    "X_train = X[:int(X.shape[1]*0.3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import *\n",
    "from pylab import *\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "from torch.nn.functional import soft_margin_loss\n",
    "\n",
    "\n",
    "class StagerNet(Module):\n",
    "    \"\"\"\n",
    "     StagerNet implementation.    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, num_channels, temp_lenght ):\n",
    "        super().__init__()      \n",
    "        # create conversion layer\n",
    "        self.relu = ReLU()\n",
    "        self.spatial_conv = Conv2d(1, num_channels, (num_channels,1), stride= (1,1))\n",
    "        self.temp_conv1 =  Conv2d(1, 16, (1,51), stride= (1,1), padding= (0,(51-1)//2))#(51-1)//2 to insure same padding\n",
    "        self.batch_norm1 = BatchNorm2d(16)\n",
    "        self.temp_conv2 =  Conv2d(16, 16, (1,51), stride= (1,1), padding= (0,(51-1)//2))#(51-1)//2 to insure same padding\n",
    "        self.batch_norm2 = BatchNorm2d(16)\n",
    "        self.maxPool = MaxPool2d((1, 13), stride=(1, 13))\n",
    "        self.flatten = Flatten()\n",
    "        self.dropout = Dropout(p = 0.5)\n",
    "        self.linear_class = Linear(num_channels*(temp_lenght//(13*13))*16,num_classes )        \n",
    "    def forward(self, inputs):\n",
    "      x = self.spatial_conv(inputs)\n",
    "      x = x.permute(0,2,1,3)\n",
    "      x = self.relu(self.temp_conv1(x))\n",
    "      # a relu activation is used before batch_norm, is it the case in the original implementation ?\n",
    "      x = self.batch_norm1(x) \n",
    "      x = self.relu(x)      \n",
    "      x = self.maxPool(x)\n",
    "      x = self.relu(self.temp_conv2(x)) \n",
    "      x = self.batch_norm2 (x)\n",
    "      x = self.relu(x)\n",
    "      x = self.maxPool(x)\n",
    "      x = self.dropout(self.flatten(x))\n",
    "      x = self.linear_class(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "class ShallowNet(Module):\n",
    "    \"\"\"\n",
    "     ShallowNet implementation.    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, num_channels , temp_lenght):\n",
    "        super().__init__()      \n",
    "        # create conversion layer\n",
    "        self.eps = 1e-45\n",
    "        self.relu = ReLU()\n",
    "        \n",
    "        self.temp_conv1 =  Conv2d(1, 40, (1,25), stride= (1,1))\n",
    "        self.batch_norm2 = BatchNorm2d(40)\n",
    "        self.spatial_conv = Conv2d(40, 40, (num_channels,1), stride= (1,1))\n",
    "        self.meanPool = AvgPool2d((1, 75), stride=(1, 15))\n",
    "        self.flatten = Flatten()\n",
    "        self.dropout = Dropout(p = 0.5)\n",
    "        self.num_features = (((temp_lenght-25+1)-75)//15+1)*40\n",
    "        self.linear_class = Linear( self.num_features,num_classes )  \n",
    "\n",
    "    def forward(self, inputs):\n",
    "      x = self.temp_conv1(inputs)\n",
    "      x = self.batch_norm2(x)\n",
    "      x = self.spatial_conv(x)\n",
    "      x = torch.pow(x, 2) # squaring non-linearity\n",
    "\n",
    "      x = self.meanPool(x)\n",
    "      x = self.flatten(x)\n",
    "      x = torch.log(x+self.eps) #log  non-linearity \n",
    "      \n",
    "      x = self.dropout(self.flatten(x))\n",
    "      x = self.linear_class(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import torch \n",
    "from torch.nn import *\n",
    "\n",
    "class WeightedSampler(torch.utils.data.sampler.Sampler):\n",
    "    r\"\"\"Sample des windows randomly\n",
    "    Arguments:\n",
    "    ---------\n",
    "        dataset (Dataset): dataset to sample from\n",
    "        size (int): The total number of sequences to sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,dataset, batch_size,size,  weights):\n",
    "    \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.size = size\n",
    "        self.dataset = dataset\n",
    "        self.serie_len = len(self.dataset)\n",
    "        \n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        num_batches = self.size// self.batch_size\n",
    "        while num_batches > 0:\n",
    "            #print()\n",
    "            sampled = 0\n",
    "            while sampled < self.batch_size:\n",
    "                target  = 2*torch.multinomial(\n",
    "            self.weights, 1, replacement=True) -1\n",
    "                t = choice(arange(0, self.serie_len-self.dataset.temp_len, 1))\n",
    "                t_ = self.dataset.get_pos(t) if target>0 else self.dataset.get_neg(t)\n",
    "                sampled += 1\n",
    "                yield (t, t_, target)\n",
    "            \n",
    "            num_batches -=1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_list)   \n",
    "\n",
    "\n",
    "class Abstract_Dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Classe dataset  pour les differents sampling\n",
    "    '''\n",
    "    def __init__(self, time_series, temp_len , n_features):\n",
    "\n",
    "        self.time_series = time_series\n",
    "        self.temp_len = temp_len\n",
    "        self.n_features = n_features\n",
    "    def get_windows(self,index):\n",
    "        '''\n",
    "        a method to load  a sequence \n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "    def get_pos(self, t_anchor):\n",
    "        '''\n",
    "        a method to get positive samples\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "    def get_neg(self, t_anchor):\n",
    "        '''\n",
    "       a method to get negative samples\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "    def get_targets(self, index):\n",
    "        '''\n",
    "        a method to get labels\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "    def __getitem__(self, index):\n",
    "        windows = self.get_windows(index)\n",
    "        target = self.get_targets(index)\n",
    "        return windows, target\n",
    "    def __len__(self): return self.time_series.shape[1]\n",
    "\n",
    "class RP_Dataset(Abstract_Dataset):\n",
    "    \n",
    "    def __init__(self, time_series, sampling_params, temp_len , n_features ):\n",
    "      super().__init__(time_series, temp_len = temp_len, n_features = n_features)\n",
    "      self.pos , self.neg = sampling_params\n",
    "    def get_windows(self,index):\n",
    "        '''\n",
    "        a method to get sampled windows\n",
    "        '''\n",
    "        (t, t_ , _) = index\n",
    "        anchor_wind = self.time_series[:,t:t+self.temp_len]\n",
    "        neg_wind = self.time_series[:,t_:t_+self.temp_len] # could be negative or positive\n",
    "        return (anchor_wind, neg_wind)\n",
    "    \n",
    "    def get_targets(self, index):\n",
    "        return index[-1]\n",
    "    def get_pos(self, t_anchor):\n",
    "\n",
    "      start = min(0,t_anchor-self.pos ) \n",
    "      end = min(self.__len__()-self.temp_len,t_anchor+self.pos ) # to get a sequence of lenght self.temp_lenght\n",
    "      t_ = choice(arange(start,end, 1)) \n",
    "      return t_\n",
    "    def get_neg(self, t_anchor):\n",
    "      \n",
    "      left_idx = arange(0, max(0, t_anchor - self.neg), 1)\n",
    "      right_idx =arange(min(self.__len__()-self.temp_len, t_anchor + self.neg),self.__len__()-self.temp_len ,1)\n",
    "      t_ = choice(hstack([left_idx, right_idx]))\n",
    "      return t_\n",
    "\n",
    "def collate(batch):\n",
    "    \n",
    "    anchors = torch.stack([torch.from_numpy(item[0][0]) for item in batch])\n",
    "    try:\n",
    "        sampled = torch.stack([torch.from_numpy(item[0][1]) for item in batch])\n",
    "    except:\n",
    "        print(batch)\n",
    "    targets = torch.stack([item[1] for item in batch])\n",
    "    \n",
    "    return (anchors, sampled), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-30cc09fc696f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_dataset =  RP_Dataset(X_train, sampling_params = (600, 1000), temp_len = T ,\n\u001b[0m\u001b[1;32m      2\u001b[0m                             n_features = C )\n\u001b[1;32m      3\u001b[0m train_sampler = WeightedSampler(train_dataset, batch_size = 30 ,size = 1000,  \n\u001b[1;32m      4\u001b[0m                           weights = [0.5, 0.5])\n\u001b[1;32m      5\u001b[0m test_sampler = WeightedSampler(test_dataset, batch_size = 30 ,size = 300,  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset =  RP_Dataset(X_train, sampling_params = (600, 1000), temp_len = T ,\n",
    "                            n_features = C )\n",
    "train_sampler = WeightedSampler(train_dataset, batch_size = 30 ,size = 1000,  \n",
    "                          weights = [0.5, 0.5])\n",
    "test_sampler = WeightedSampler(test_dataset, batch_size = 30 ,size = 300,  \n",
    "                          weights = [0.5, 0.5])\n",
    "samplers = {\"train\" : train_sampler, \"val\": test_sampler}\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, num_workers=0,sampler = samplers[\"train\"], collate_fn=collate)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for (anchors, sampled), targets in train_loader: \n",
    "      anchors = anchors\n",
    "    print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990043\n"
     ]
    }
   ],
   "source": [
    "t_anchor = 991043\n",
    "print(t_anchor - test_dataset.neg)\n",
    "left_idx = arange(0, max(0, t_anchor - test_dataset.neg), 1)\n",
    "right_idx =arange(min(test_dataset.__len__()-test_dataset.temp_len, t_anchor + test_dataset.neg),test_dataset.__len__()-test_dataset.temp_len ,1)\n",
    "t_ = choice(hstack([left_idx, right_idx]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 69, 200])\n",
      "tensor([[-1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [-1],\n",
      "        [-1]])\n",
      "torch.Size([5, 69, 200])\n",
      "tensor([[-1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [-1],\n",
      "        [ 1]])\n"
     ]
    }
   ],
   "source": [
    "rp_dataset = RP_Dataset(X, sampling_params = (600, 1000), temp_len = 200 , n_features = 69 )\n",
    "sampler = WeightedSampler(rp_dataset, batch_size = 5 ,size = 10,  weights = [0.5, 0.5])\n",
    "loader = torch.utils.data.DataLoader(rp_dataset, batch_size=5,  sampler=sampler,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=collate)\n",
    "\n",
    "for (anchors, sampled), targets in loader: \n",
    "  print(sampled.shape)\n",
    "  print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e754495c3e87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mEEG_FeatureExtractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEEG_FeatureExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sampled' is not defined"
     ]
    }
   ],
   "source": [
    "EEG_FeatureExtractor = StagerNet(num_classes = 10, num_channels = 69, temp_lenght = 200 )\n",
    "from pylab import *\n",
    "\n",
    "inputs = sampled.unsqueeze(dim = 1)\n",
    "EEG_FeatureExtractor.to(float)\n",
    "y = EEG_FeatureExtractor(inputs)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import soft_margin_loss\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class Relative_Positioning(nn.Module):\n",
    "  def __init__(self, EEG_FeatureExtractor, C, T, embedding_dim=100):\n",
    "    super().__init__()\n",
    "    self.feature_extractor = EEG_FeatureExtractor(num_classes =embedding_dim , num_channels = C , temp_lenght = T).to(float).to(device)\n",
    "    #self.feature_extractor.float()\n",
    "    self.linear = nn.Linear(embedding_dim, 1)\n",
    "    self.loss_fn = nn.SoftMarginLoss()\n",
    "\n",
    "  def forward(self, x):\n",
    "    first_samples = x[0].unsqueeze(dim=1)\n",
    "    second_samples = x[1].unsqueeze(dim=1)\n",
    "\n",
    "    h_first = self.feature_extractor(first_samples)\n",
    "    h_second = self.feature_extractor(second_samples)\n",
    "\n",
    "    h_combined = torch.abs(h_first - h_second)\n",
    "\n",
    "    out = self.linear(h_combined)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relative_Positioning(\n",
       "  (feature_extractor): StagerNet(\n",
       "    (relu): ReLU()\n",
       "    (spatial_conv): Conv2d(1, 69, kernel_size=(69, 1), stride=(1, 1))\n",
       "    (temp_conv1): Conv2d(1, 16, kernel_size=(1, 51), stride=(1, 1), padding=(0, 25))\n",
       "    (batch_norm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (temp_conv2): Conv2d(16, 16, kernel_size=(1, 51), stride=(1, 1), padding=(0, 25))\n",
       "    (batch_norm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (maxPool): MaxPool2d(kernel_size=(1, 13), stride=(1, 13), padding=0, dilation=1, ceil_mode=False)\n",
       "    (flatten): Flatten()\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear_class): Linear(in_features=1104, out_features=100, bias=True)\n",
       "  )\n",
       "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (loss_fn): SoftMarginLoss()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssl_model = Relative_Positioning(StagerNet,C = 69, T = 200 )\n",
    "ssl_model.to(float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0201],\n",
       "        [ 1.6412],\n",
       "        [ 0.2893],\n",
       "        [ 1.0355],\n",
       "        [ 1.6782]], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssl_model(x = (anchors.to(device), sampled.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (anchors, sampled), y in data_loader:\n",
    "            x = (anchors.to(device).to(float).contiguous(), sampled.to(device).to(float).contiguous())\n",
    "            y = y.to(device).to(float).contiguous()\n",
    "            loss = rp_loss(model, x, y)\n",
    "            total_loss += loss * x.shape[0]\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def rp_loss(model, x, y):\n",
    "\tout = model(x)\n",
    "\treturn soft_margin_loss(out, y)\n",
    " \n",
    "def _train(model, train_loader, optimizer, epoch):\n",
    "\tmodel.train()\n",
    "\t\n",
    "\ttrain_losses = []\n",
    "\tfor (anchors, sampled), y in train_loader:\n",
    "\t\tx = (anchors.to(device).to(float).contiguous(), sampled.to(device).to(float).contiguous())\n",
    "\t\ty = y.to(device).to(float).contiguous()\n",
    "\t\tloss = rp_loss(model, x, y)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\ttrain_losses.append(loss.item())\n",
    "\treturn train_losses\n",
    "\n",
    "def _eval_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (anchors, sampled), y in data_loader:\n",
    "            x = (anchors.to(device).to(float).contiguous(), sampled.to(device).to(float).contiguous())\n",
    "            y = y.to(device).to(float).contiguous()\n",
    "            loss = rp_loss(model, x, y)\n",
    "            total_loss += loss * x[0].shape[0]\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss.item()\n",
    "\n",
    "\t\n",
    "\n",
    "saved_models_dir = \"saved_models\"\n",
    "def _train_epochs(model, train_loader, test_loader, train_args):\n",
    "\tepochs, lr = train_args['epochs'], train_args['lr']\n",
    "\toptimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\tif not os.path.exists(saved_models_dir):\n",
    "\t\tos.makedirs(saved_models_dir)\n",
    "\t\n",
    "\ttrain_losses = []\n",
    "\ttest_losses = [_eval_loss(model, test_loader)]\n",
    "\tfor epoch in range(1, epochs+1):\n",
    "\t\tmodel.train()\n",
    "\t\ttrain_losses.extend(_train(model, train_loader, optimizer, epoch))\n",
    "\t\ttest_loss = _eval_loss(model, test_loader)\n",
    "\t\ttest_losses.append(test_loss)\n",
    "\t\tprint(f'Epoch {epoch}, Test loss {test_loss:.4f}')\n",
    "\t\t\n",
    "\t\t# save model every 10 epochs\n",
    "\t\tif epoch % 2 == 0:\n",
    "\t\t\ttorch.save(model.state_dict(), os.path.join(ROOT, 'saved_models', 'ssl_model_epoch{}.pt'.format(epoch)))\n",
    "\ttorch.save(model.state_dict(), os.path.join(root, 'saved_models', 'ssl_model.pt'))\n",
    "\treturn train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_ssl(model, train_dataset, test_dataset,sampler, n_epochs=20, lr=1e-3, batch_size=256, load_last_saved_model=False, num_workers=8):\n",
    "\tC = train_dataset.__getitem__((0, 0,1))[0][0].shape[0] # num channels\n",
    "\tT = train_dataset.__getitem__((0, 0,1))[0][0].shape[1] # num timepoints\n",
    "\t\n",
    "\tif load_last_saved_model:\n",
    "\t\tmodel.load_state_dict(torch.load(os.path.join(root, 'saved_models', 'ssl_model.pt')))\n",
    "\n",
    "\tif torch.cuda.device_count() > 1:\n",
    "\t\tmodel = nn.DataParallel(model)\n",
    "        \n",
    "\tmodel.to(device)\n",
    "    \n",
    "   \n",
    "\n",
    "\ttrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers,sampler = sampler[\"train\"], collate_fn=collate)\n",
    "\ttest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers,sampler = sampler[\"val\"], collate_fn=collate)\n",
    "\n",
    "\tnew_train_losses, new_test_losses = _train_epochs(model, train_loader, test_loader, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t dict(epochs=n_epochs, lr=lr))\n",
    "\n",
    "\tif load_last_saved_model:\n",
    "\t\ttrain_losses, test_losses = load_losses(saved_models_dir, 'ssl')\n",
    "\telse:\n",
    "\t\ttrain_losses = []\n",
    "\t\ttest_losses = []\n",
    "\t\n",
    "\ttrain_losses.extend(new_train_losses)\n",
    "\ttest_losses.extend(new_test_losses)\n",
    "\n",
    "\tsave_losses(train_losses, test_losses, saved_models_dir, 'ssl')\n",
    "\n",
    "\treturn train_losses, test_losses, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"n_features\" : 69,\n",
    "    \"temp_lenght\": 200,\n",
    "    \"sampling_params\" : {\"pos\": 600, \"neg\": 1000},\n",
    "    \"batch_size\": 30,\n",
    "    \"train_size\": 1000,\n",
    "    \"test_size\": 300,\n",
    "    \"sampling_weights\": [0.5, 0.5],\n",
    "    \"n_epochs\" : 15,\n",
    "    \"lr\": 1e-3\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Test loss 0.0003\n",
      "Epoch 2, Test loss 0.0002\n",
      "Epoch 3, Test loss 0.0008\n",
      "Epoch 4, Test loss 0.0004\n",
      "Epoch 5, Test loss 0.0002\n",
      "Epoch 6, Test loss 0.0003\n",
      "Epoch 7, Test loss 0.0003\n",
      "Epoch 8, Test loss 0.0002\n",
      "Epoch 9, Test loss 0.0003\n",
      "Epoch 10, Test loss 0.0002\n",
      "Epoch 11, Test loss 0.0002\n",
      "Epoch 12, Test loss 0.0002\n",
      "Epoch 13, Test loss 0.0002\n",
      "Epoch 14, Test loss 0.0002\n",
      "Epoch 15, Test loss 0.0002\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f171ebdb58fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m train_losses, test_losses, model = train_ssl(ssl_model, train_dataset, test_dataset,\n\u001b[1;32m     35\u001b[0m                                              \u001b[0msamplers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                                              load_last_saved_model=False, num_workers= 0)\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamplers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-11a968563dc6>\u001b[0m in \u001b[0;36mtrain_ssl\u001b[0;34m(model, train_dataset, test_dataset, sampler, n_epochs, lr, batch_size, load_last_saved_model, num_workers)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mtest_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_test_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0msave_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_models_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ssl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_losses' is not defined"
     ]
    }
   ],
   "source": [
    "#def get_neg(self, t_anchor):\n",
    "      \n",
    "    #left_idx = arange(0, max(0, t_anchor - self.neg), 1)\n",
    "    #right_idx =arange(min(self.__len__()-self.temp_len, t_anchor + self.neg),self.__len__()-self.temp_len ,1)\n",
    "    #try :\n",
    "       # t_ = choice(hstack([left_idx, right_idx]))\n",
    "    #except:\n",
    "        #print(\"t_\", t_anchor)\n",
    "                \n",
    "    #return t_\n",
    "#RP_Dataset.get_neg = get_neg\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "root = ROOT\n",
    "\n",
    "C = 69\n",
    "T = 200\n",
    "split = int(X.shape[1]*0.6)\n",
    "X_train = X[:, :split]\n",
    "X_test = X[:, split:]\n",
    "ssl_model = Relative_Positioning(StagerNet,C , T )\n",
    "ssl_model.to(float)\n",
    "train_dataset =  RP_Dataset(X_train, sampling_params = (600, 1000), temp_len = T ,\n",
    "                            n_features = C )\n",
    "test_dataset =  RP_Dataset(X_test, sampling_params = (600, 1000), temp_len = T ,\n",
    "                            n_features = C )\n",
    "train_sampler = WeightedSampler(train_dataset, batch_size = 30 ,size = 1000,  \n",
    "                          weights = [0.5, 0.5])\n",
    "test_sampler = WeightedSampler(test_dataset, batch_size = 30 ,size = 300,  \n",
    "                          weights = [0.5, 0.5])\n",
    "samplers = {\"train\" : train_sampler, \"val\": test_sampler}\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, num_workers=0,sampler = samplers[\"val\"], collate_fn=collate)\n",
    "\n",
    "train_losses, test_losses, model = train_ssl(ssl_model, train_dataset, test_dataset,\n",
    "                                             samplers,n_epochs=15, lr=1e-3,batch_size=30, \n",
    "                                             load_last_saved_model=False, num_workers= 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters:\n",
      "======================\n",
      "- subject: 01\n",
      "- C : 69\n",
      "- T : 1000\n",
      "- epochs : 5\n",
      "- batch size : 30\n",
      "- lr : 0.001\n",
      "- n_train : 3000\n",
      "- n_test : 300\n",
      "- pos : 600\n",
      "- neg : 600\n",
      "Opening raw data file /home/brain/openmiir/raw_data/P01-raw.fif...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 64)  idle\n",
      "    Range : 0 ... 2478165 =      0.000 ...  4840.166 secs\n",
      "Ready.\n",
      "Reading 0 ... 2478165  =      0.000 ...  4840.166 secs...\n",
      "Reading /home/brain/openmiir/eeg/preprocessing/ica/P01-100p_64c-ica.fif ...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 64)  idle\n",
      "Now restoring ICA solution ...\n",
      "Ready.\n",
      "Transforming to ICA space (61 components)\n",
      "Zeroing out 4 ICA components\n",
      "Epoch 1, Test loss 0.0004\n",
      "Epoch 2, Test loss 0.0007\n",
      "Epoch 3, Test loss 0.0003\n",
      "Epoch 4, Test loss 0.0005\n",
      "Epoch 5, Test loss 0.0003\n"
     ]
    }
   ],
   "source": [
    "!python3 main.py --epochs 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
